{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci4Hxe5GJk4c"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub openai chromadb tiktoken faiss-cpu\n",
        "!pip -q install sentence_transformers pypdf\n",
        "!pip -q install -U FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-x\""
      ],
      "metadata": {
        "id": "Tq8iVHTyJtVy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "## Text Splitting & Docloader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "NjpBHVxEJzyl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비"
      ],
      "metadata": {
        "id": "TAKrytrfJ32z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
        "!unzip -q new_papers.zip -d new_papers"
      ],
      "metadata": {
        "id": "ljd1P5q2MHqo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader('./new_papers/new_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "yqy0qOkrJ2Ns"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WlqVBr9NZDG",
        "outputId": "cd62ea61-57ad-46de-dbe0-fffcafb8d931"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = documents[:10]  # 일부만 사용"
      ],
      "metadata": {
        "id": "pIPwDYrqNqx3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "8oH6VNn-OGdD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e967XOtJOJnj",
        "outputId": "182bb0f1-d01f-45e0-917e-0a952e4ca30e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='FlashAttention : Fast and Memory-Eﬃcient Exact Attention\\nwith IO-Awareness\\nTri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher Réy\\nyDepartment of Computer Science, Stanford University\\nzDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY\\n{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\\nchrismre@cs.stanford.edu\\nJune 24, 2022\\nAbstract\\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity\\nof self-attention are quadratic in sequence length. Approximate attention methods have attempted\\nto address this problem by trading oﬀ model quality to reduce the compute complexity, but often do\\nnot achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\\naware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes', metadata={'source': 'new_papers/new_papers/Flash-attention.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BGE Embeddings"
      ],
      "metadata": {
        "id": "sdfhEUSwKItH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "EnM5kKDdJ9Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for printing docs\n",
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
      ],
      "metadata": {
        "id": "HVm_9nXPKNbK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = FAISS.from_documents(texts,\n",
        "                                 bge_embeddings\n",
        "                                #  OpenAIEmbeddings()\n",
        "                                 ).as_retriever()\n",
        "\n",
        "docs = retriever.get_relevant_documents(\"What is AliBi?\")\n",
        "#lets look at the docs\n",
        "pretty_print_docs(docs[2:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpiTF3A-KXYC",
        "outputId": "5c761bf7-3cf3-4c64-dafe-1995fc94bcfe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "Our implementation uses Apex’s FMHA code ( https://github.com/NVIDIA/apex/tree/master/apex/\n",
            "contrib/csrc/fmha ) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA\n",
            "implementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu,\n",
            "Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and\n",
            "suggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of\n",
            "their attention algorithm.\n",
            "We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.\n",
            "CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under\n",
            "No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak\n",
            "Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).\n",
            "In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is,\n",
            "carefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU\n",
            "on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left). On modern\n",
            "1arXiv:2205.14135v2  [cs.LG]  23 Jun 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding contextual compression with an LLMChainExtractor\n",
        "\n",
        "Now let's wrap our base retriever with a ContextualCompressionRetriever. We'll add an LLMChainExtractor, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
      ],
      "metadata": {
        "id": "yYstutCVQIy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# make the compressor\n",
        "llm = OpenAI(temperature=0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "# it needs a base retriever (we're using FAISS Retriever) and a compressor (Made above)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                       base_retriever=retriever)"
      ],
      "metadata": {
        "id": "4LOn7PU6Ol6U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compressor prompt\n",
        "compressor.llm_chain.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxResQ9kQVbh",
        "outputId": "f397c779-e958-4e89-dbb3-1f3fdaee6c31"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], output_parser=NoOutputParser(), template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_docs = compression_retriever.get_relevant_documents(\"What is FlashAttention?\")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poNmABLsQ-9Z",
        "outputId": "1e6e1071-3b52-4c2e-f3d0-74f508d88376"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "- FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities.\n",
            "- We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13].\n",
            "- FlashAttention enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K).\n",
            "- Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance performance on Path-256.\n",
            "- FlashAttention is up to 3\u0002faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K.\n",
            "- Up to sequence length of 512, FlashAttention is both faster and more memory-eﬃcient than any existing attention method.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "FlashAttentionMemory Hierarchy with\n",
            "Bandwidth & Memory SizeAttention on GPT-2\n",
            "FlashAttention PyTorchTime (ms)\n",
            "MatmulMaskSoftmaxDropoutMatmul\n",
            "Fused\n",
            "KernelQ: N x d V: N X dKT: d x N\n",
            "QKT: N x N\n",
            "sm(Q KT)V: N x dOuter Loop\n",
            "Copy Block to SRAM\n",
            "CopyOuter Loop\n",
            "CopyInner LoopCompute Block\n",
            "on SRAM\n",
            "Output to HBM\n",
            "Inner LoopInner LoopOuter Loop\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 3:\n",
            "\n",
            "FlashAttention speeds up the long-range arena (LRA) benchmark 2.4 \u0002.\n",
            "•Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt-\n",
            "tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length\n",
            "1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-\n",
            "document classiﬁcation tasks. Finally, FlashAttention yields the ﬁrst Transformer that can achieve\n",
            "better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse\n",
            "FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random\n",
            "performance on Path-256 (sequence length 64K).\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 4:\n",
            "\n",
            "FlashAttention : Fast and Memory-Eﬃcient Exact Attention\n",
            "with IO-Awareness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More built-in compressors: filters\n",
        "\n",
        "### LLMChainFilter\n",
        "\n",
        "Uses an LLM chain to select out the queries to show the final LLM - This could be shown to a model fine tuned to do this\n",
        "\n",
        "\"YES\" we show it or \"NO\" we don't show it"
      ],
      "metadata": {
        "id": "EqX9x3VsS6Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RCAFQKjDT4i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "\n",
        "_filter = LLMChainFilter.from_llm(llm)\n",
        "_filter.llm_chain.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULloirjeSb9F",
        "outputId": "ed145b61-9baa-4b9e-c60d-f5faa7710d90"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], output_parser=BooleanOutputParser(), template=\"Given the following question and context, return YES if the context is relevant to the question and NO if it isn't.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\n> Relevant (YES / NO):\")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever = ContextualCompressionRetriever(base_compressor=_filter, base_retriever=retriever)\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\"What is FlashAttention?\")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sqMH0sWS-3Q",
        "outputId": "ec446906-c15c-49ae-b189-66d28b4520dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "•Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves\n",
            "their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\n",
            "6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\n",
            "enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,\n",
            "solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\n",
            "to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\n",
            "performance on Path-256.\n",
            "•Benchmarking Attention. FlashAttention is up to 3\u0002faster than the standard attention implemen-\n",
            "tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\n",
            "FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "FlashAttentionMemory Hierarchy with\n",
            "Bandwidth & Memory SizeAttention on GPT-2\n",
            "FlashAttention PyTorchTime (ms)\n",
            "MatmulMaskSoftmaxDropoutMatmul\n",
            "Fused\n",
            "KernelQ: N x d V: N X dKT: d x N\n",
            "QKT: N x N\n",
            "sm(Q KT)V: N x dOuter Loop\n",
            "Copy Block to SRAM\n",
            "CopyOuter Loop\n",
            "CopyInner LoopCompute Block\n",
            "on SRAM\n",
            "Output to HBM\n",
            "Inner LoopInner LoopOuter Loop\n",
            "GPU\n",
            "SRAM\n",
            "GPU\n",
            "HBM\n",
            "Main Memory\n",
            "(CPU DRAM)SRAM : 19 TB/s (20 MB)\n",
            "HBM: 1.5 TB/s (40 GB)\n",
            "DRAM : 12.8 GB/s\n",
            "                (>1 TB)\n",
            "051015Figure 1: Left: FlashAttention uses tiling to prevent materialization of the large 𝑁\u0002𝑁attention matrix\n",
            "(dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through\n",
            "blocks of the KandVmatrices and loads them to fast on-chip SRAM. In each block, FlashAttention\n",
            "loops over blocks of Qmatrix (blue arrows), loading them to SRAM, and writing the output of the attention\n",
            "computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 3:\n",
            "\n",
            "speeds up GPT-2 up to 3 \u0002over HuggingFace [ 87] and 18\u0002over Megatron [ 77] over standard Transformers.\n",
            "FlashAttention speeds up the long-range arena (LRA) benchmark 2.4 \u0002.\n",
            "•Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt-\n",
            "tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length\n",
            "1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-\n",
            "document classiﬁcation tasks. Finally, FlashAttention yields the ﬁrst Transformer that can achieve\n",
            "better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse\n",
            "FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random\n",
            "performance on Path-256 (sequence length 64K).\n",
            "•Benchmarking Attention. We measure the runtime and memory performance of FlashAttention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 4:\n",
            "\n",
            "FlashAttention : Fast and Memory-Eﬃcient Exact Attention\n",
            "with IO-Awareness\n",
            "Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher Réy\n",
            "yDepartment of Computer Science, Stanford University\n",
            "zDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY\n",
            "{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\n",
            "chrismre@cs.stanford.edu\n",
            "June 24, 2022\n",
            "Abstract\n",
            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity\n",
            "of self-attention are quadratic in sequence length. Approximate attention methods have attempted\n",
            "to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do\n",
            "not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\n",
            "aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\n",
            "an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmbeddingsFilter\n",
        "Use an Embedding model to filter out the results that are closest to the query"
      ],
      "metadata": {
        "id": "lISXKBXIT70g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "\n",
        "embeddings = OpenAIEmbeddings() # base retriever에서 사용했던 BGE embedding 과는 다른 것 사용\n",
        "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\"What is FlashAttention?\")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "id": "3ZJxgDLlT8cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines\n",
        "\n",
        "\n",
        "### Stringing compressors and document transformers together\n",
        "\n",
        "DocumentCompressorPipeline allows us to string things together.\n",
        "\n",
        "BaseDocumentTransformers - can do transformations on the docs -eg. split the text and\n",
        "\n",
        "EmbeddingsRedundantFilter - filter out what is not related after a split or transformation\n"
      ],
      "metadata": {
        "id": "RaCmBczXVBA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# 300 token 단위로 자른다.(.을 spliter로 해서 문장 단위로 절삭되지 않게 고려)\n",
        "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
        "\n",
        "# N  개의 embed vector 사이의 pairwise cosine similarity를 계산해서(N*N),\n",
        "# thr보다 높은 쌍을 찾고\n",
        "# (first, second) 에서 second를 중복으로 보고 drop\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "\n",
        "# user_query의 embed와 N  개의 embed vector 사이의 pairwise cosine similarity 측정\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
        "\n",
        "## making the pipeline\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    transformers=[splitter, redundant_filter, relevant_filter]\n",
        ")"
      ],
      "metadata": {
        "id": "3_XuEZ3mUzH7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
        "                                                       base_retriever=retriever)\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\"What is FlashAttention?\")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU2XZejmYCDa",
        "outputId": "e94dd6c2-5b23-4f70-f718-e0c4e901cbbb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 608, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 446, which is longer than the specified 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "FlashAttention is up to 3\u0002faster than the standard attention implemen-\n",
            "tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\n",
            "FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "We measure the runtime and memory performance of FlashAttention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 3:\n",
            "\n",
            "Finally, FlashAttention yields the ﬁrst Transformer that can achieve\n",
            "better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse\n",
            "FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random\n",
            "performance on Path-256 (sequence length 64K).\n",
            "•Benchmarking Attention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 4:\n",
            "\n",
            "speeds up GPT-2 up to 3 \u0002over HuggingFace [ 87] and 18\u0002over Megatron [ 77] over standard Transformers.\n",
            "FlashAttention speeds up the long-range arena (LRA) benchmark 2.4 \u0002.\n",
            "•Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 5:\n",
            "\n",
            "We argue that a missing principle is making attention algorithms IO-\n",
            "aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\n",
            "an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 6:\n",
            "\n",
            "•Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves\n",
            "their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\n",
            "6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 7:\n",
            "\n",
            "FlashAttention : Fast and Memory-Eﬃcient Exact Attention\n",
            "with IO-Awareness\n",
            "Tri Daoy, Daniel Y\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 8:\n",
            "\n",
            "FlashAttention\n",
            "enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,\n",
            "solely from using a longer sequence length (16K)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 9:\n",
            "\n",
            "FlashAt-\n",
            "tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length\n",
            "1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-\n",
            "document classiﬁcation tasks\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 10:\n",
            "\n",
            "In the outer loop (red arrows), FlashAttention loops through\n",
            "blocks of the KandVmatrices and loads them to fast on-chip SRAM. In each block, FlashAttention\n",
            "loops over blocks of Qmatrix (blue arrows), loading them to SRAM, and writing the output of the attention\n",
            "computation back to HBM\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 11:\n",
            "\n",
            "FlashAttentionMemory Hierarchy with\n",
            "Bandwidth & Memory SizeAttention on GPT-2\n",
            "FlashAttention PyTorchTime (ms)\n",
            "MatmulMaskSoftmaxDropoutMatmul\n",
            "Fused\n",
            "KernelQ: N x d V: N X dKT: d x N\n",
            "QKT: N x N\n",
            "sm(Q KT)V: N x dOuter Loop\n",
            "Copy Block to SRAM\n",
            "CopyOuter Loop\n",
            "CopyInner LoopCompute Block\n",
            "on SRAM\n",
            "Output to HBM\n",
            "Inner LoopInner LoopOuter Loop\n",
            "GPU\n",
            "SRAM\n",
            "GPU\n",
            "HBM\n",
            "Main Memory\n",
            "(CPU DRAM)SRAM : 19 TB/s (20 MB)\n",
            "HBM: 1.5 TB/s (40 GB)\n",
            "DRAM : 12.8 GB/s\n",
            "                (>1 TB)\n",
            "051015Figure 1: Left: FlashAttention uses tiling to prevent materialization of the large 𝑁\u0002𝑁attention matrix\n",
            "(dotted box) on (relatively) slow GPU HBM\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 12:\n",
            "\n",
            "Approximate attention methods have attempted\n",
            "to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do\n",
            "not achieve wall-clock speedup\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 13:\n",
            "\n",
            "Right:Speedup over the PyTorch implementation of attention on GPT-2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### different pipeline\n",
        "\n",
        "## making the pipeline\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    transformers=[splitter, compressor, redundant_filter, relevant_filter]\n",
        ")\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
        "                                                       base_retriever=retriever)\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\"What is FlashAttention?\")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSPZOX0qYH56",
        "outputId": "9e677e1c-b85e-4c56-87a1-d512c36b099b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 608, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 446, which is longer than the specified 300\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "FlashAttention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "FlashAttention is up to 3\u0002faster than the standard attention implemen-\n",
            "tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\n",
            "FlashAttention is both faster and more memory-eﬃcient than any existing attention method\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 3:\n",
            "\n",
            "FlashAttention : Fast and Memory-Eﬃcient Exact Attention\n",
            "with IO-Awareness\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 4:\n",
            "\n",
            "FlashAttention yields the ﬁrst Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K).\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 5:\n",
            "\n",
            "FlashAttention speeds up the long-range arena (LRA) benchmark 2.4 \u0002.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 6:\n",
            "\n",
            "FlashAttention enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge, solely from using a longer sequence length (16K)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 7:\n",
            "\n",
            "Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance performance on Path-256.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 8:\n",
            "\n",
            "FlashAttention loops through blocks of the KandVmatrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Qmatrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 9:\n",
            "\n",
            "FlashAt-\n",
            "tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length\n",
            "1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-\n",
            "document classiﬁcation tasks\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 10:\n",
            "\n",
            "PyTorch implementation of attention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 11:\n",
            "\n",
            "FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 12:\n",
            "\n",
            "FlashAttentionMemory Hierarchy with\n",
            "Bandwidth & Memory SizeAttention on GPT-2\n",
            "FlashAttention PyTorchTime (ms)\n",
            "MatmulMaskSoftmaxDropoutMatmul\n",
            "Fused\n",
            "KernelQ: N x d V: N X dKT: d x N\n",
            "QKT: N x N\n",
            "sm(Q KT)V: N x dOuter Loop\n",
            "Copy Block to SRAM\n",
            "CopyOuter Loop\n",
            "CopyInner LoopCompute Block\n",
            "on SRAM\n",
            "Output to HBM\n",
            "Inner LoopInner LoopOuter Loop\n",
            "GPU\n",
            "SRAM\n",
            "GPU\n",
            "HBM\n",
            "Main Memory\n",
            "(CPU DRAM)SRAM : 19 TB/s (20 MB)\n",
            "HBM: 1.5 TB/s (40 GB)\n",
            "DRAM : 12.8 GB/s\n",
            "                (>1 TB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mo4e5-yYSHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}