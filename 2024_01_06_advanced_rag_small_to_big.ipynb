{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG 01: Small to Big\n",
        "\n",
        "### Child-Parent RecursiveRetriever and Sentence Window Retrieval with LlamaIndex\n",
        "\n",
        "Sources:\n",
        "- https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html\n",
        "- https://docs.llamaindex.ai/en/latest/examples/node_postprocessor/MetadataReplacementDemo.html"
      ],
      "metadata": {
        "id": "_UTq1VN3yX4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "SwbZ3WpXyNNx",
        "outputId": "3679928f-0242-4c6f-8bce-3b0fbcb6addc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 autoevals-0.0.41 beautifulsoup4-4.12.2 braintrust-0.0.87 braintrust-core-0.0.7 chevron-0.14.0 dataclasses-json-0.6.3 deprecated-1.2.14 gitdb-4.0.11 h11-0.14.0 html2text-2020.1.16 httpcore-1.0.2 httpx-0.26.0 levenshtein-0.23.0 llama_hub-0.0.66 llama_index-0.9.25.post1 marshmallow-3.20.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 openai-1.6.1 pillow-10.2.0 pyaml-23.12.0 pypdf-3.17.4 rapidfuzz-3.6.1 retrying-1.3.4 smmap-5.0.1 tiktoken-0.5.2 torch-2.1.2 torchvision-0.16.2 transformers-4.36.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install -U llama_hub llama_index braintrust autoevals pypdf pillow transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "c4wujtJUyf4D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RAG Review"
      ],
      "metadata": {
        "id": "0k3p6wjuynFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from llama_hub.file.pdf.base import PDFReader\n",
        "from llama_index.response.notebook_utils import display_source_node\n",
        "from llama_index.retrievers import RecursiveRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "import json"
      ],
      "metadata": {
        "id": "QaFOwn6HykKT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Loading Documents"
      ],
      "metadata": {
        "id": "YeZWbLYdyzNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PDFReader()\n",
        "docs0 = loader.load_data(file=Path(\"llama2.pdf\"))"
      ],
      "metadata": {
        "id": "5HGdJnODykcK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs0[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PFtdiMz0K9",
        "outputId": "1ce3d8bd-c730-493a-e694-701b54d82841"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='d335aed7-2244-4285-85bb-7ca7795386c9', embedding=None, metadata={'page_label': '1', 'file_name': 'llama2.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='018b304d342200869f05b9e3bcb41cf91a368cd743da48c666fb67110a4a2c1f', text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import Document\n",
        "\n",
        "doc_text = \"\\n\\n\\n\".join([d.get_content() for d in docs0]) # into single text\n",
        "docs = [Document(text=doc_text)]"
      ],
      "metadata": {
        "id": "yq3bD35L0URn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Parsing Documents into Text Chunks (Nodes)\n",
        "\n",
        "```\n",
        "SimpleNodeParser에서의 chunk 전략\n",
        "1. 먼저 하나의 text를 여러 개의 split으로 쪼갠다.\n",
        "  split 쪼개는 원리  : 먼저 paragraph로 쪼개지면 종료, 아니라면 sentence로 쪼개기, 아니라면 정규식으로 쪼개기, 아니라면 word로 쪼개기, 아니라면 char로 쪼개기\n",
        "    1. split by paragraph separator\n",
        "    2. split by chunking tokenizer (default is nltk sentence tokenizer)\n",
        "    3. split by second chunking regex (default is \"[^,\\.;]+[,\\.;]?\")\n",
        "    4. split by default separator (\" \")\n",
        "\n",
        "2. split들을 chunk_size를 고려해서 merge 한다.\n",
        "  이를 통해서 하나의 split으로 들어갈 것이 여러 chunk로 fragment되는 것을 나름 방지한다.\n",
        "  여기서 chunk_size 는 char 단위가 아니라 token_size로 봐야 한다.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "LcrSlUoi1RB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.schema import IndexNode\n",
        "\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
        "base_nodes = node_parser.get_nodes_from_documents(docs)\n",
        "for idx, node in enumerate(base_nodes):\n",
        "  node.id_ = f\"node-{idx}\"  # replace id_ from hash-like to number\n",
        "len(base_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMy0-YbT01CP",
        "outputId": "ae6d516d-3baf-4fb4-db1c-c092cf6f5147"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_nodes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMqW55ljtpbR",
        "outputId": "b3c67f02-9e98-4464-f50c-dbbc6289a5fe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextNode(id_='node-0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a7705984-64ba-4a8b-8d84-2196a137671f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='398cb8798614de7e8a14cfff4b34f87a9f979d4dfaac81358c311ca67a659fe0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a3bf5ad9-a072-441c-a9be-7fcf8c4d6894', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='820de41da4f942046105d6d061c52e7e04429023a554953b03b420aaebdb53ae')}, hash='b203c05456cf61d73bf4a69ecb61de212829d2a81f128ed71dc7612ec85ece86', text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\\n\\n\\nContents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . .', start_char_idx=0, end_char_idx=2563, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Select Embedding Model and LLM"
      ],
      "metadata": {
        "id": "9vXWIqZ851V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import resolve_embed_model\n",
        "\n",
        "embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V44i5l6r02jU",
        "outputId": "1269efc4-6563-4d44-a118-b7d0e9b83cef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create Index, retriever, and query engine"
      ],
      "metadata": {
        "id": "PQs7U5Oej0WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_index = VectorStoreIndex(base_nodes, service_context=service_context)\n",
        "base_retriever = base_index.as_retriever(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "hJXaaKwr6MDd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrievals = base_retriever.retrieve(\n",
        "    \"Can you tell me about the key concepts for safety finetuning\"\n",
        ")\n",
        "\n",
        "for n in retrievals:\n",
        "    display_source_node(n, source_length=1500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "1todFfRFj-g2",
        "outputId": "f9ea81cd-ee57-4e9b-81a1-ce8f1af4406a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-26<br>**Similarity:** 0.8541543380274644<br>**Text:** TruthfulQA ↑ToxiGen ↓\nMPT7B 29.13 22.32\n30B 35.25 22.61\nFalcon7B 25.95 14.53\n40B 40.39 23.44\nLlama 17B 27.42 23.00\n13B 41.74 23.08\n33B 44.19 22.57\n65B 48.71 21.77\nLlama 27B 33.29 21.25\n13B 41.86 26.10\n34B 43.45 21.19\n70B 50.18 24.60\nTable 11: Evaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the\npercentageofgenerationsthatarebothtruthfulandinformative(thehigherthebetter). ForToxiGen,we\npresent the percentage of toxic generations (the smaller, the better).\nBenchmarks give a summary view ofmodel capabilities and behaviors that allow us to understand general\npatternsinthemodel,buttheydonotprovideafullycomprehensiveviewoftheimpactthemodelmayhave\nonpeopleorreal-worldoutcomes;thatwouldrequirestudyofend-to-endproductdeployments. Further\ntesting and mitigation should be done to understand bias and other social issues for the specific context\nin which a system may be deployed. For this, it may be necessary to test beyond the groups available in\ntheBOLDdataset(race,religion,andgender). AsLLMsareintegratedanddeployed,welookforwardto\ncontinuing research that will amplify their potential for positive impact on these important social issues.\n4.2 Safety Fine-Tuning\nIn this section, we describe our approach to safety fine-tuning, including safety categories, annotation\nguidelines,andthetechniquesweusetomitigatesafetyrisks. Weemployaprocesssimilartothegeneral\nfine-tuning methods as described in Section 3, with some notable differences related to safet...<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-27<br>**Similarity:** 0.8464193261082225<br>**Text:** advice). The attackvectors exploredconsist ofpsychological manipulation(e.g., authoritymanipulation),\nlogic manipulation (e.g., false premises), syntactic manipulation (e.g., misspelling), semantic manipulation\n(e.g., metaphor), perspective manipulation (e.g., role playing), non-English languages, and others.\nWethendefinebestpracticesforsafeandhelpfulmodelresponses: themodelshouldfirstaddressimmediate\nsafetyconcernsifapplicable,thenaddressthepromptbyexplainingthepotentialriskstotheuser,andfinally\nprovide additional information if possible. We also ask the annotators to avoid negative user experience\ncategories (see Appendix A.5.2). The guidelines are meant to be a general guide for the model and are\niteratively refined and revised to include newly identified risks.\n4.2.2 Safety Supervised Fine-Tuning\nInaccordancewiththeestablishedguidelinesfromSection4.2.1,wegatherpromptsanddemonstrations\nofsafemodelresponsesfromtrainedannotators,andusethedataforsupervisedfine-tuninginthesame\nmanner as described in Section 3.1. An example can be found in Table 5.\nThe annotators are instructed to initially come up with prompts that they think could potentially induce\nthemodel toexhibit unsafebehavior, i.e.,perform redteaming, asdefined bythe guidelines. Subsequently,\nannotators are tasked with crafting a safe and helpful response that the model should produce.\n4.2.3 Safety RLHF\nWeobserveearlyinthedevelopmentof Llama 2-Chat thatitisabletogeneralizefromthesafedemonstrations\ninsupervisedfine-t...<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine_base = RetrieverQueryEngine.from_args(\n",
        "    base_retriever, service_context=service_context\n",
        ")\n",
        "\n",
        "response = query_engine_base.query(\n",
        "    \"Can you tell me about the key concepts for safety finetuning\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEY8rTLFkZgS",
        "outputId": "b5ae5899-7e0e-4e93-d861-6f5d8b8776ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. \n",
            "\n",
            "In supervised safety fine-tuning, adversarial prompts and safe demonstrations are gathered and included in the general supervised fine-tuning process. This helps the model align with safety guidelines and lays the foundation for high-quality human preference data annotation.\n",
            "\n",
            "Safety RLHF involves integrating safety in the general RLHF pipeline. This includes training a safety-specific reward model and gathering more challenging adversarial prompts for rejection sampling style fine-tuning and PPO (Proximal Policy Optimization) optimization.\n",
            "\n",
            "Safety context distillation is the final step in safety fine-tuning. It involves generating safer model responses by prefixing a prompt with a safety preprompt, such as \"You are a safe and responsible assistant.\" The model is then fine-tuned on the safer responses without the preprompt, effectively distilling the safety preprompt (context) into the model.\n",
            "\n",
            "These concepts are used to mitigate safety risks and improve the safety performance of language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk References: Smaller Child Chunks Referring to Bigger Parent Chunk"
      ],
      "metadata": {
        "id": "iHtU25aBlZ3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_chunk_sizes = [128, 256, 512]\n",
        "sub_node_parsers = [  # overlap은 20%\n",
        "    SimpleNodeParser.from_defaults(chunk_size=c, chunk_overlap=int(c*0.2)) for c in sub_chunk_sizes\n",
        "]\n",
        "\n",
        "all_nodes = []\n",
        "for base_node in base_nodes:  # 기존 larger chunk(1024)로 된 node들에 대해서 smaller chunk로 만든다.\n",
        "  for n in sub_node_parsers:\n",
        "    sub_nodes = n.get_nodes_from_documents([base_node]) # text node\n",
        "    sub_inodes = [  # index node를 이용해서 smaller node 가 larger node를 pointing하게 한다.\n",
        "        IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
        "    ]\n",
        "    all_nodes.extend(sub_inodes)\n",
        "\n",
        "  # also add original node to node\n",
        "  original_inode = IndexNode.from_text_node(base_node, base_node.node_id)\n",
        "  all_nodes.append(original_inode)\n",
        "\n",
        "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
        "len(all_nodes_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ejp2yqlZbu",
        "outputId": "31b3fafc-0c0e-40ae-da8b-bcf1d49981a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1648"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_nodes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB-Szi1SkpyG",
        "outputId": "a3eac687-2e86-4b95-c511-37e681e40b21"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IndexNode(id_='7b1b798d-e4f6-4ea7-9073-303bfc4a6dc3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='node-0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b203c05456cf61d73bf4a69ecb61de212829d2a81f128ed71dc7612ec85ece86'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c59ae255-48f5-465d-88d4-73fc3e764bdd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c1cd7a3cd4e6b6f0bf126aec76d521214773696e2cc43ab2e814cffbb76582cf')}, hash='846d3ce8d8b3f2b35dce5dc8a696ccbded7b43b16619f156e69bcba19c7706e9', text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui', start_char_idx=0, end_char_idx=412, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index_chunk = VectorStoreIndex(  # 100개에 2분 소요, 1600개면 30여분\n",
        "    all_nodes, service_context=service_context\n",
        ")\n",
        "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "9FG3w4afoMK0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_chunk = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
        "    node_dict=all_nodes_dict,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "nodes = retriever_chunk.retrieve(\n",
        "    \"Can you tell me about the key concepts for safety finetuning\"\n",
        ")\n",
        "for node in nodes:\n",
        "    display_source_node(node, source_length=2000)"
      ],
      "metadata": {
        "id": "FFW0StVsor-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
        "    retriever_chunk, service_context=service_context\n",
        ")\n",
        "response = query_engine_chunk.query(\n",
        "    \"Can you tell me about the key concepts for safety finetuning\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "lnOpnEmppfZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "zleLMGU6qszo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA pair 만들기 (by generate_qa_embedding_pairs)\n",
        "\n",
        "```\n",
        "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
        "Context information is below.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Given the context information and not prior knowledge.\n",
        "generate only questions based on the below query.\n",
        "\n",
        "You are a Teacher/ Professor. Your task is to setup \\\n",
        "{num_questions_per_chunk} questions for an upcoming \\\n",
        "quiz/examination. The questions should be diverse in nature \\\n",
        "across the document. Restrict the questions to the \\\n",
        "context information provided.\"\n",
        "\"\"\"\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3kp7I6S0qwP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import (\n",
        "    generate_question_context_pairs,\n",
        "    EmbeddingQAFinetuneDataset,\n",
        ")\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "qE3urrVOqvFJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_nodes_sample[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r5KJkoeubFd",
        "outputId": "f0a7c1a6-c3e4-4745-e98e-824b987f1bf7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextNode(id_='node-0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a7705984-64ba-4a8b-8d84-2196a137671f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='398cb8798614de7e8a14cfff4b34f87a9f979d4dfaac81358c311ca67a659fe0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a3bf5ad9-a072-441c-a9be-7fcf8c4d6894', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='820de41da4f942046105d6d061c52e7e04429023a554953b03b420aaebdb53ae')}, hash='b203c05456cf61d73bf4a69ecb61de212829d2a81f128ed71dc7612ec85ece86', text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\\n\\n\\nContents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . .', start_char_idx=0, end_char_idx=2563, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_nodes_sample = base_nodes[:5]  # 일부만 사용, 1개의 node context 당 2개의 question 생성\n",
        "eval_dataset = generate_question_context_pairs(base_nodes_sample,\n",
        "                                               llm=2,\n",
        "                                               num_questions_per_chunk=2)  # EmbeddingQAFinetuneDataset (query, relevant_doc)"
      ],
      "metadata": {
        "id": "iMOBnz4grHnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset.save_json(\"llama2_eval_dataset.json\")"
      ],
      "metadata": {
        "id": "ebY6rpCdrQht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_index.evaluation import RetrieverEvaluator, get_retrieval_results_df\n",
        "\n",
        "# set vector retriever similarity top k to higher\n",
        "top_k = 10\n",
        "\n",
        "\n",
        "def display_results(names, results_arr):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    hit_rates = []\n",
        "    mrrs = []\n",
        "    for name, eval_results in zip(names, results_arr):\n",
        "        metric_dicts = []\n",
        "        for eval_result in eval_results:\n",
        "            metric_dict = eval_result.metric_vals_dict\n",
        "            metric_dicts.append(metric_dict)\n",
        "        results_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "        hit_rate = results_df[\"hit_rate\"].mean()\n",
        "        mrr = results_df[\"mrr\"].mean()\n",
        "        hit_rates.append(hit_rate)\n",
        "        mrrs.append(mrr)\n",
        "\n",
        "    final_df = pd.DataFrame(\n",
        "        {\"retrievers\": names, \"hit_rate\": hit_rates, \"mrr\": mrrs}\n",
        "    )\n",
        "    display(final_df)\n"
      ],
      "metadata": {
        "id": "0taj-4ZPrWls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base\n",
        "base_retriever = base_index.as_retriever(similarity_top_k=top_k)\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=base_retriever\n",
        ")\n",
        "results_base = await retriever_evaluator.aevaluate_dataset(\n",
        "    eval_dataset, show_progress=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "zGnUhXX6vLNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk\n",
        "vector_retriever_chunk = vector_index_chunk.as_retriever(\n",
        "    similarity_top_k=top_k\n",
        ")\n",
        "retriever_chunk = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
        "    node_dict=all_nodes_dict,\n",
        "    verbose=True,\n",
        ")\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever_chunk\n",
        ")\n",
        "\n",
        "results_chunk = await retriever_evaluator.aevaluate_dataset(\n",
        "    eval_dataset, show_progress=True\n",
        ")"
      ],
      "metadata": {
        "id": "O-QtXxOGvNPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results_df = get_retrieval_results_df(\n",
        "    [\n",
        "        \"Base Retriever\",\n",
        "        \"Retriever (Chunk References)\"\n",
        "    ],\n",
        "    [results_base, results_chunk],\n",
        ")\n",
        "display(full_results_df)"
      ],
      "metadata": {
        "id": "4Jm6MXbsvQpU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}