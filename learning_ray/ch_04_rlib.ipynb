{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b55bf1-df5f-406a-a76c-9a9ba9f1ab30",
   "metadata": {},
   "source": [
    "## Environment 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3168777-ce99-4ab8-87ac-f07f2f8f5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self, num_actions: int):\n",
    "        \"\"\" Discrete action space for num_actions.\n",
    "        Discrete(4) can be used as encoding moving in one of the cardinal directions.\n",
    "        \"\"\"\n",
    "        self.n = num_actions\n",
    "\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n - 1)\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    seeker, goal = (0, 0), (4, 4)\n",
    "    info = {'seeker': seeker, 'goal': goal}\n",
    "\n",
    "    def __init__(self,  *args, **kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset seeker and goal positions, return observations.\"\"\"\n",
    "        self.seeker = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return 5 * self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal\"\"\"\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"We're done if we found the goal\"\"\"\n",
    "        return self.seeker == self.goal\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in a direction and return all available information.\"\"\"\n",
    "        if action == 0:  # move down\n",
    "            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\n",
    "        elif action == 1:  # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1] - 1, 0))\n",
    "        elif action == 2:  # move up\n",
    "            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\n",
    "        elif action == 3:  # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1] + 1, 4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        return self.get_observation(), self.get_reward(), self.is_done(), self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        try:\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "\n",
    "class GymEnvironment(Environment, gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Make our original `Environment` a gym `Env`.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "gym_env = GymEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c896e5a-c99b-4499-b83a-a08b0b7ef6c0",
   "metadata": {},
   "source": [
    "## Rllib Python 으로 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5d85f3-e7e4-402f-b940-34c6ffc0cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/tune/logger/tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n",
      "2024-03-09 13:31:11,779\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=42461)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "\u001b[2m\u001b[36m(pid=42461)\u001b[0m   from pkg_resources import packaging\n",
      "\u001b[2m\u001b[36m(pid=42461)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/tune/logger/tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\u001b[2m\u001b[36m(pid=42461)\u001b[0m   VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n",
      "\u001b[2m\u001b[36m(pid=42462)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "\u001b[2m\u001b[36m(pid=42462)\u001b[0m   from pkg_resources import packaging\n",
      "\u001b[2m\u001b[36m(pid=42462)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/tune/logger/tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\u001b[2m\u001b[36m(pid=42462)\u001b[0m   VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42461)\u001b[0m 2024-03-09 13:31:15,754\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42461)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/rllib/models/catalog.py:810: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42461)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42461)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py:70: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42461)\u001b[0m   super(DistributionalQTFModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42462)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/rllib/models/catalog.py:810: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42462)\u001b[0m   prep = cls(observation_space, options)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42462)\u001b[0m /data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py:70: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42462)\u001b[0m   super(DistributionalQTFModel, self).__init__(\n",
      "2024-03-09 13:31:16,546\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "/data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/rllib/models/catalog.py:810: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  prep = cls(observation_space, options)\n",
      "/data/anaconda3/envs/ray/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/distributional_q_tf_model.py:70: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  super(DistributionalQTFModel, self).__init__(\n",
      "2024-03-09 13:31:20,495\tINFO trainable.py:172 -- Trainable.setup took 10.737 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-03-09 13:31:20,496\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "2024-03-09 13:31:20,563\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "2024-03-09 13:31:21,822\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 10000\n",
      "counters:\n",
      "  last_target_update_ts: 9502\n",
      "  num_agent_steps_sampled: 10000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 10000\n",
      "  num_env_steps_trained: 144000\n",
      "  num_target_updates: 18\n",
      "custom_metrics: {}\n",
      "date: 2024-03-09_13-32-36\n",
      "done: false\n",
      "episode_len_mean: 11.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.0\n",
      "episode_reward_mean: 1.0\n",
      "episode_reward_min: 1.0\n",
      "episodes_this_iter: 95\n",
      "episodes_total: 354\n",
      "experiment_id: 986d8e86711c49c389c43a7e65b5cff7\n",
      "hostname: hoondori-ML\n",
      "info:\n",
      "  last_target_update_ts: 9502\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 4499.0\n",
      "      learner_stats:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 0.9902322888374329\n",
      "        mean_q: 0.9334491491317749\n",
      "        mean_td_error: -0.0003340337425470352\n",
      "        min_q: 0.8533341884613037\n",
      "        model: {}\n",
      "      num_agent_steps_trained: 32.0\n",
      "      num_grad_updates_lifetime: 4500.0\n",
      "      td_error: [-0.00018143653869628906, -0.000559389591217041, -0.0010064244270324707,\n",
      "        -0.001530289649963379, 5.716085433959961e-05, 0.00141221284866333, -0.00015228986740112305,\n",
      "        -0.00047475099563598633, -0.0033263564109802246, -0.00012546777725219727,\n",
      "        -2.7954578399658203e-05, -0.0012947916984558105, -0.00013113021850585938,\n",
      "        -2.7954578399658203e-05, -0.001006782054901123, 0.0002700686454772949, -0.00048035383224487305,\n",
      "        -0.0002842545509338379, 0.0003012418746948242, -0.00013113021850585938, 0.001351475715637207,\n",
      "        -0.00018143653869628906, 0.00258558988571167, -0.00047665834426879883, 0.00141221284866333,\n",
      "        -0.0012947916984558105, -0.0008571743965148926, -0.0012947916984558105, -0.00024169683456420898,\n",
      "        -0.00047665834426879883, -0.000984787940979004, -0.001530289649963379]\n",
      "  num_agent_steps_sampled: 10000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 10000\n",
      "  num_env_steps_trained: 144000\n",
      "  num_target_updates: 18\n",
      "iterations_since_restore: 10\n",
      "node_ip: 172.30.1.49\n",
      "num_agent_steps_sampled: 10000\n",
      "num_agent_steps_trained: 144000\n",
      "num_env_steps_sampled: 10000\n",
      "num_env_steps_sampled_this_iter: 1000\n",
      "num_env_steps_trained: 144000\n",
      "num_env_steps_trained_this_iter: 16000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 2\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 16000\n",
      "perf:\n",
      "  cpu_util_percent: 13.641666666666666\n",
      "  ram_util_percent: 58.01666666666666\n",
      "pid: 40443\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05625609254309394\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02902750200172162\n",
      "  mean_inference_ms: 0.5046022313573586\n",
      "  mean_raw_obs_processing_ms: 0.4912408709523038\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 11.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 95\n",
      "  hist_stats:\n",
      "    episode_lengths: [8, 36, 9, 11, 16, 17, 9, 8, 10, 12, 16, 10, 12, 11, 10, 11,\n",
      "      8, 8, 34, 9, 12, 19, 8, 17, 8, 10, 8, 8, 26, 12, 8, 8, 8, 8, 9, 8, 8, 8, 8,\n",
      "      8, 8, 8, 8, 10, 8, 8, 8, 8, 9, 8, 8, 8, 8, 10, 18, 12, 8, 10, 8, 19, 12, 9,\n",
      "      12, 8, 8, 9, 12, 35, 39, 8, 19, 10, 8, 8, 10, 34, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "      8, 8, 8, 8, 8, 12, 8, 8, 8, 10, 8, 8, 8, 8, 8]\n",
      "    episode_reward: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05625609254309394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02902750200172162\n",
      "    mean_inference_ms: 0.5046022313573586\n",
      "    mean_raw_obs_processing_ms: 0.4912408709523038\n",
      "time_since_restore: 75.9364881515503\n",
      "time_this_iter_s: 8.032250881195068\n",
      "time_total_s: 75.9364881515503\n",
      "timers:\n",
      "  learn_throughput: 12064.19\n",
      "  learn_time_ms: 2.652\n",
      "  load_throughput: 327920.176\n",
      "  load_time_ms: 0.098\n",
      "  synch_weights_time_ms: 1.584\n",
      "  training_iteration_time_ms: 15.978\n",
      "timestamp: 1709958756\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 10000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "warmup_time: 10.738039016723633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "config = (DQNConfig().environment(GymEnvironment)\n",
    "          .rollouts(num_rollout_workers=2, create_env_on_local_worker=True))\n",
    "\n",
    "pretty_print(config.to_dict())\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62f2f5-f9b2-43eb-901e-fb90ef8f5750",
   "metadata": {},
   "source": [
    "## 모델의 저장과 로드, 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195b3290-9297-48bd-8d14-1490aca2f2f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoondori/ray_results/DQN_GymEnvironment_2024-03-09_11-04-28d_cn6mha/checkpoint_000010\n",
      "evaluation:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths: []\n",
      "    episode_reward: []\n",
      "  num_agent_steps_sampled_this_iter: 10\n",
      "  num_env_steps_sampled_this_iter: 10\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  timesteps_this_iter: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms import Algorithm\n",
    "\n",
    "ckpt = algo.save()\n",
    "print(ckpt)\n",
    "\n",
    "eval = algo.evaluate()\n",
    "print(pretty_print(eval))\n",
    "\n",
    "#algo.stop()\n",
    "#restored_algo = Algorithm.from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a94b49-b110-4342-b375-22c41c6eeb60",
   "metadata": {},
   "source": [
    "## 액션 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce6789d-b2dc-49d2-bee7-3f5c11b1563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "env = GymEnvironment()\n",
    "done = False\n",
    "total_reward = 0\n",
    "observations = env.reset()\n",
    "while not done:\n",
    "    action = algo.compute_single_action(observations)\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "print(total_reward)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c771e7e8-3469-48a5-904f-34ee226fe822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obs_1': 3, 'obs_2': 3}\n"
     ]
    }
   ],
   "source": [
    "action = algo.compute_actions({\"obs_1\":observations, \"obs_2\":observations})\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae2d99-d68c-4126-9270-b2c648bec719",
   "metadata": {},
   "source": [
    "## 정책과 모델 상태에 접근하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e93bbf-9a8d-4ff8-b793-32d6d2bd9393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " observations (InputLayer)   [(None, 25)]                 0         []                            \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                (None, 256)                  6656      ['observations[0][0]']        \n",
      "                                                                                                  \n",
      " fc_out (Dense)              (None, 256)                  65792     ['fc_1[0][0]']                \n",
      "                                                                                                  \n",
      " value_out (Dense)           (None, 1)                    257       ['fc_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 72705 (284.00 KB)\n",
      "Trainable params: 72705 (284.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy()\n",
    "weights = policy.get_weights()\n",
    "model = policy.model\n",
    "model.base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d31d68-0d56-462d-84aa-90d647eff24a",
   "metadata": {},
   "source": [
    "## worker 별 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e0c59e-9434-4289-9afa-a5381e11cd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "workers = algo.workers\n",
    "weight_list = workers.foreach_worker(\n",
    "    lambda remote_trainer: remote_trainer.get_policy().get_weights()\n",
    ")\n",
    "print(len(weight_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e8ce1-d289-4d12-b16d-ba9119865c37",
   "metadata": {},
   "source": [
    "# 실험 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d87703-783f-4c8b-b1c0-1c6280f99cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리소스 구성\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "config = DQNConfig().resources(num_gpus=1, num_cpus_per_worker=2, num_gpus_per_worker=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34f29364-4d57-455f-99a4-04a917e4b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 롤아웃 워커 구성\n",
    "\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "config = DQNConfig().rollouts(\n",
    "    num_rollout_workers=4,\n",
    "    num_envs_per_worker=1,\n",
    "    create_env_on_local_worker=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f57d4128-6db0-433a-99ee-199164530743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 구성\n",
    "\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "config = DQNConfig().environment(\n",
    "    env=\"CartPole-v1\",\n",
    "    env_config={\"my_config\": \"value\"},\n",
    "    observation_space=None,\n",
    "    action_space=None,\n",
    "    render_env=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff26939-28b4-491e-9bbf-86f644e518cf",
   "metadata": {},
   "source": [
    "## 다중 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc1a413-28dd-494c-99c1-e20707e0a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete\n",
    "import os\n",
    "\n",
    "class MultiAgentMaze(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self,  *args, **kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
    "        self.goal = (4, 4)\n",
    "        self.info = {1: {'obs': self.agents[1]}, 2: {'obs': self.agents[2]}}\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
    "\n",
    "        return {1: self.get_observation(1), 2: self.get_observation(2)}\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "        seeker = self.agents[agent_id]\n",
    "        return 5 * seeker[0] + seeker[1]\n",
    "\n",
    "    def get_reward(self, agent_id):\n",
    "        return 1 if self.agents[agent_id] == self.goal else 0\n",
    "\n",
    "    def is_done(self, agent_id):\n",
    "        return self.agents[agent_id] == self.goal\n",
    "\n",
    "    def step(self, action):\n",
    "        agent_ids = action.keys()\n",
    "\n",
    "        for agent_id in agent_ids:\n",
    "            seeker = self.agents[agent_id]\n",
    "            if action[agent_id] == 0:  # move down\n",
    "                seeker = (min(seeker[0] + 1, 4), seeker[1])\n",
    "            elif action[agent_id] == 1:  # move left\n",
    "                seeker = (seeker[0], max(seeker[1] - 1, 0))\n",
    "            elif action[agent_id] == 2:  # move up\n",
    "                seeker = (max(seeker[0] - 1, 0), seeker[1])\n",
    "            elif action[agent_id] == 3:  # move right\n",
    "                seeker = (seeker[0], min(seeker[1] + 1, 4))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "            self.agents[agent_id] = seeker\n",
    "\n",
    "        observations = {i: self.get_observation(i) for i in agent_ids}\n",
    "        rewards = {i: self.get_reward(i) for i in agent_ids}\n",
    "        done = {i: self.is_done(i) for i in agent_ids}\n",
    "\n",
    "        done[\"__all__\"] = all(done.values())\n",
    "\n",
    "        return observations, rewards, done, self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"We override this method here so clear the output in Jupyter notebooks.\n",
    "        The previous implementation works well in the terminal, but does not clear\n",
    "        the screen in interactive environments.\n",
    "        \"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        try:\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.agents[1][0]][self.agents[1][1]] = '|1'\n",
    "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
    "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a891b3-8ffe-470c-9b3c-fca7252f54b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | |2| | |\n",
      "| | | | | |\n",
      "| | | | |1|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = MultiAgentMaze()\n",
    "\n",
    "while True:\n",
    "    obs, rew, done, info = env.step(\n",
    "        {1: env.action_space.sample(), 2: env.action_space.sample()}\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    if any(done.values()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f1977-aaea-4bfb-a494-2fac0311a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.23.5  <-- 에러 발생시 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b275074b-f06a-4385-92d9-bdd0cd46e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:57:17,820\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {},\n",
       "  'num_env_steps_sampled': 1000,\n",
       "  'num_env_steps_trained': 0,\n",
       "  'num_agent_steps_sampled': 2000,\n",
       "  'num_agent_steps_trained': 0},\n",
       " 'sampler_results': {'episode_reward_max': 2.0,\n",
       "  'episode_reward_min': 2.0,\n",
       "  'episode_reward_mean': 2.0,\n",
       "  'episode_len_mean': 134.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 7,\n",
       "  'policy_reward_min': {'policy_1': 1.0, 'policy_2': 1.0},\n",
       "  'policy_reward_max': {'policy_1': 1.0, 'policy_2': 1.0},\n",
       "  'policy_reward_mean': {'policy_1': 1.0, 'policy_2': 1.0},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0],\n",
       "   'episode_lengths': [245, 33, 43, 209, 24, 33, 351],\n",
       "   'policy_policy_1_reward': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "   'policy_policy_2_reward': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.7600655684342514,\n",
       "   'mean_inference_ms': 3.307951556576359,\n",
       "   'mean_action_processing_ms': 0.05153509286733775,\n",
       "   'mean_env_wait_ms': 0.020791481543968728,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0},\n",
       " 'episode_reward_max': 2.0,\n",
       " 'episode_reward_min': 2.0,\n",
       " 'episode_reward_mean': 2.0,\n",
       " 'episode_len_mean': 134.0,\n",
       " 'episodes_this_iter': 7,\n",
       " 'policy_reward_min': {'policy_1': 1.0, 'policy_2': 1.0},\n",
       " 'policy_reward_max': {'policy_1': 1.0, 'policy_2': 1.0},\n",
       " 'policy_reward_mean': {'policy_1': 1.0, 'policy_2': 1.0},\n",
       " 'hist_stats': {'episode_reward': [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0],\n",
       "  'episode_lengths': [245, 33, 43, 209, 24, 33, 351],\n",
       "  'policy_policy_1_reward': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  'policy_policy_2_reward': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7600655684342514,\n",
       "  'mean_inference_ms': 3.307951556576359,\n",
       "  'mean_action_processing_ms': 0.05153509286733775,\n",
       "  'mean_env_wait_ms': 0.020791481543968728,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'num_healthy_workers': 0,\n",
       " 'num_in_flight_async_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 2000,\n",
       " 'num_agent_steps_trained': 0,\n",
       " 'num_env_steps_sampled': 1000,\n",
       " 'num_env_steps_trained': 0,\n",
       " 'num_env_steps_sampled_this_iter': 1000,\n",
       " 'num_env_steps_trained_this_iter': 0,\n",
       " 'timesteps_total': 1000,\n",
       " 'num_steps_trained_this_iter': 0,\n",
       " 'agent_timesteps_total': 2000,\n",
       " 'timers': {'training_iteration_time_ms': 2.306},\n",
       " 'counters': {'num_env_steps_sampled': 1000,\n",
       "  'num_env_steps_trained': 0,\n",
       "  'num_agent_steps_sampled': 2000,\n",
       "  'num_agent_steps_trained': 0},\n",
       " 'done': False,\n",
       " 'episodes_total': 7,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '7b0cf1cceeb0455f9d6c640ff9a90c6c',\n",
       " 'date': '2024-03-09_13-57-22',\n",
       " 'timestamp': 1709960242,\n",
       " 'time_this_iter_s': 4.749458312988281,\n",
       " 'time_total_s': 4.749458312988281,\n",
       " 'pid': 133484,\n",
       " 'hostname': 'hoondori-ML',\n",
       " 'node_ip': '172.30.1.49',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': __main__.MultiAgentMaze,\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': False,\n",
       "  'enable_connectors': False,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.0005,\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.02,\n",
       "   'epsilon_timesteps': 10000},\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 1000,\n",
       "  'export_native_model_files': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'worker_cls': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  'simple_optimizer': False,\n",
       "  'replay_sequence_length': None,\n",
       "  'target_network_update_freq': 500,\n",
       "  'replay_buffer_config': {'type': ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer,\n",
       "   'prioritized_replay': -1,\n",
       "   'capacity': 50000,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'replay_sequence_length': 1,\n",
       "   'worker_side_prioritization': False},\n",
       "  'num_steps_sampled_before_learning_starts': 1000,\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'grad_clip': 40,\n",
       "  'tau': 1.0,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': False,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': True,\n",
       "  'hiddens': [256],\n",
       "  'double_q': True,\n",
       "  'n_step': 1,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'td_error_loss_fn': 'huber',\n",
       "  'categorical_distribution_temperature': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'policy_1': (None,\n",
       "     Discrete(25),\n",
       "     Discrete(4),\n",
       "     {'gamma': 0.8}),\n",
       "    'policy_2': (None, Discrete(25), Discrete(4), {'gamma': 0.95})},\n",
       "   'policy_mapping_fn': <function __main__.<lambda>(agent_id)>,\n",
       "   'policies_to_train': None,\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'count_steps_by': 'env_steps',\n",
       "   'observation_fn': None},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'tf',\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_workers': 0},\n",
       " 'time_since_restore': 4.749458312988281,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 1,\n",
       " 'warmup_time': 1.5741803646087646,\n",
       " 'perf': {'cpu_util_percent': 10.171428571428573,\n",
       "  'ram_util_percent': 55.35714285714285}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 agent가 각자의 정책을 가지도록 한 후에 학습\n",
    "\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "algo = DQNConfig()\\\n",
    "    .environment(env=MultiAgentMaze)\\\n",
    "    .multi_agent(\n",
    "        policies = {\n",
    "            \"policy_1\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
    "            ),\n",
    "            \"policy_2\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
    "            ),            \n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",\n",
    "    ).build()\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f64834-19f3-4c49-b22b-9e2a5a603c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
