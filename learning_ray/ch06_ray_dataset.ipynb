{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2314454c-2384-4ee0-a084-5ce526bee1d3",
   "metadata": {},
   "source": [
    "# 레이 데이터셋을 활용한 데이터 분산 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f00ee6-f73f-47e8-be58-1b6e04b554bc",
   "metadata": {},
   "source": [
    "### 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6d2886-d7c7-476a-b926-dff7d2d179d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9d020-d7bc-4884-bb02-22f48f1ec47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of ImportError: cannot import name 'ExtensionArrayFormatter\n",
    "!pip install pandas==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e38d45-4027-47f4-be12-cf5eba1a43c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 12:35:07,266\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[0, 1, 2, 3, 4]\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "ds = ray.data.range(10000)\n",
    "\n",
    "print(ds.count())\n",
    "print(ds.take(5))\n",
    "print(ds.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb6da1-a6b5-424d-be78-832a733aca3c",
   "metadata": {},
   "source": [
    "### 스토리지에 읽고 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62bb771-05af-4fae-8959-f5945a81e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|███████████████████████████████████| 33/33 [00:00<00:00, 1737.60it/s]\n",
      "Repartition: 100%|███████████████████████████████| 5/5 [00:00<00:00, 906.25it/s]\n",
      "Write Progress: 100%|███████████████████████████| 5/5 [00:00<00:00, 1736.34it/s]\n",
      "Read progress: 100%|████████████████████████████| 5/5 [00:00<00:00, 2529.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ray.data.range(10000).repartition(5).write_csv(\"local_dir\")\n",
    "ds = ray.data.read_csv(\"local_dir\")\n",
    "print(ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948212d8-e51b-472c-8119-842b7c701174",
   "metadata": {},
   "source": [
    "### 빌트인 변환 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebedfbf1-a238-41c4-a4eb-f63f5ad53f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 12:38:56,003\tWARNING dataset.py:4233 -- The `map`, `flat_map`, and `filter` operations are unvectorized and can be very slow. Consider using `.map_batches()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Filter: 100%|███████████████████████████| 66/66 [00:00<00:00, 1177.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sort Sample: 100%|███████████████████████████| 66/66 [00:00<00:00, 14758.44it/s]\n",
      "Shuffle Map: 100%|█████████████████████████████| 66/66 [00:00<00:00, 986.65it/s]\n",
      "Shuffle Reduce: 100%|██████████████████████████| 66/66 [00:00<00:00, 555.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds1 = ray.data.range(10000)\n",
    "ds2 = ray.data.range(10000)\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.count())\n",
    "\n",
    "ds3 = ds3.filter(lambda x: x % 2 == 0)\n",
    "print(ds3.count())\n",
    "\n",
    "ds3 = ds3.sort()\n",
    "print(ds3.take(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f441be-274b-4632-b711-93e7a9316c35",
   "metadata": {},
   "source": [
    "### 블록 및 파티셔닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf7f4ac-10a7-41c1-a514-588cd6545cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33\n",
      "66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|████████████████████████████████████| 66/66 [00:00<00:00, 231.95it/s]\n",
      "Repartition: 100%|██████████████████████████| 200/200 [00:00<00:00, 3337.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds1 = ray.data.range(10000)\n",
    "print(ds1.num_blocks())\n",
    "ds2 = ray.data.range(10000)\n",
    "print(ds2.num_blocks())\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.num_blocks())\n",
    "\n",
    "print(ds3.repartition(200).num_blocks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4d00e-bbad-4501-89b5-a2d03683e4f1",
   "metadata": {},
   "source": [
    "### 스키마와 데이터 포멧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80ff350-9b29-4ae9-99a4-209202372017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: string\n",
      "value: int64\n"
     ]
    }
   ],
   "source": [
    "ds = ray.data.from_items([{\"id\": \"abc\", \"value\":1}])\n",
    "print(ds.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bb2bb-64d9-4af0-976d-f4ae73c82c14",
   "metadata": {},
   "source": [
    "### 데이터셋 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41695984-f28d-4c22-a3c3-2a2054366e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map: 100%|██████████████████████████████| 33/33 [00:00<00:00, 1158.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|███████████████████████| 33/33 [00:00<00:00, 567.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 단일 행 연산\n",
    "ds = ray.data.range(10000).map(lambda x: x ** 2)\n",
    "print(ds.take(5))\n",
    "\n",
    "# vectorized 연산\n",
    "import numpy as np\n",
    "ds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\n",
    "print(ds.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36e580d0-3c91-4a86-b92f-ae88ad1fbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress (1 actors 0 pending): 100%|████████| 33/33 [00:00<00:00, 50.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ML 모델에 batch 적용\n",
    "import numpy as np\n",
    "def load_model():\n",
    "    class DummyModel:\n",
    "        def __call__(self, batch):\n",
    "            return np.square(batch).tolist()\n",
    "    return DummyModel()\n",
    "        \n",
    "class MLModel:        \n",
    "    def __init__(self):\n",
    "        self._model = load_model()\n",
    "    def __call__(self, batch):\n",
    "        return self._model(batch)\n",
    "\n",
    "ds = ray.data.range(10000)\n",
    "ds = ds.map_batches(MLModel, compute=\"actors\")\n",
    "print(ds.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de94b45-7e14-4633-bb2f-6d70206b8ab6",
   "metadata": {},
   "source": [
    "### 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee44de-1eb5-4eb4-b4d6-e8a05d5f8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this only works if you create an S3 bucket and upload the data there.\n",
    "ds = (ray.data.read_parquet(\"s3://my_bucket/input_data\")\n",
    "      .map(cpu_intensive_preprocessing)\n",
    "      .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\n",
    "      .repartition(10))\n",
    "\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b3037-643e-4caa-a93f-c4a30faa2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this only works if you create an S3 bucket and upload the data there.\n",
    "ds = (ray.data.read_parquet(\"s3://my_bucket/input_data\")\n",
    "      .window(blocks_per_window=5)\n",
    "      .map(cpu_intensive_preprocessing)\n",
    "      .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\n",
    "      .repartition(10))\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5f2df-af65-4917-8195-1cdfa2d99eb1",
   "metadata": {},
   "source": [
    "### 병렬 분류기 복사본 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0fd5532-5121-4bfe-80d7-c2c643c2b753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5 training workers\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, alpha: float):\n",
    "        self._model = SGDClassifier(alpha=alpha)\n",
    "    def train(self, train_shard: ray.data.Dataset):\n",
    "        for i, epoch in enumerate(train_shard.iter_epochs()):\n",
    "            X, Y = zip(*list(epoch.iter_rows()))\n",
    "            self._model.partial_fit(X, Y, classes=[0,1])\n",
    "        return self._model\n",
    "    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n",
    "        return self._model.score(X_test, Y_test)\n",
    "\n",
    "ALPHA_VALS = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012]\n",
    "print(f\"Starting {len(ALPHA_VALS)} training workers\")\n",
    "workers = [TrainingWorker.remote(alpha) for alpha in ALPHA_VALS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d48736c-9cde-47a2-b2d9-f34364507502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/10 [00:00<?, ?it/s]179254)\u001b[0m \n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[Ar pid=179254)\u001b[0m \n",
      "Stage 1:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A254)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  20%|██        | 2/10 [00:01<00:04,  1.63it/s]\u001b[A \n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  30%|███       | 3/10 [00:01<00:03,  1.95it/s]\u001b[A \n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  40%|████      | 4/10 [00:01<00:02,  2.77it/s]\u001b[A \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  50%|█████     | 5/10 [00:02<00:03,  1.59it/s]\u001b[A \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  60%|██████    | 6/10 [00:02<00:01,  2.18it/s]\u001b[A \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  70%|███████   | 7/10 [00:03<00:01,  1.55it/s]\u001b[A \n",
      "Stage 0: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Stage 0: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  80%|████████  | 8/10 [00:04<00:00,  2.09it/s]\u001b[A \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0:  90%|█████████ | 9/10 [00:04<00:00,  2.73it/s]\u001b[A \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 0: 100%|██████████| 10/10 [00:04<00:00,  3.44it/s][A \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=179254)\u001b[0m \n",
      "Stage 1: 100%|██████████| 10/10 [00:04<00:00,  4.16it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SGDClassifier(alpha=8e-05),\n",
       " SGDClassifier(alpha=9e-05),\n",
       " SGDClassifier(),\n",
       " SGDClassifier(alpha=0.00011),\n",
       " SGDClassifier(alpha=0.00012)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 119.67it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 84.64it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 124.85it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 83.27it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 127.76it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 84.65it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 118.84it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 82.24it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 117.50it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 82.45it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    *datasets.make_classification()\n",
    ")\n",
    "train_ds = ray.data.from_items(list(zip(X_train, Y_train)))\n",
    "shards = (train_ds.repeat(10)\n",
    "          .random_shuffle_each_window()\n",
    "          .split(len(workers), locality_hints=workers))\n",
    "ray.get([\n",
    "    worker.train.remote(shard)\n",
    "    for worker, shard in zip(workers, shards)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "418fa93e-b679-4066-b2d5-7b0f54c50aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88, 0.8, 0.88, 0.6, 0.72]\n"
     ]
    }
   ],
   "source": [
    "print(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eea998-15b5-4eed-80a7-47c0c04c8768",
   "metadata": {},
   "source": [
    "## 외부 라이브러리와 통합(dask)\n",
    "\n",
    "* !pip install dask==2022.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffd288-767a-4e4c-a219-5e40854e6f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "ray.init()\n",
    "enable_dask_on_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b76ac-5b70-4c3c-82a5-0061c4c1b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "df = dask.datasets.timeseries()\n",
    "df = df[df.y > 0].groupby(\"name\").x.std()\n",
    "df.compute()  # Trigger the task graph to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b00cc7-f4b9-409f-88ce-888df1037784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Convert the Dataset to a Dask DataFrame.\n",
    "df = ds.to_dask()\n",
    "print(df.std().compute())  # -> 2886.89568\n",
    "\n",
    "# Convert the Dask DataFrame back to a Dataset.\n",
    "ds = ray.data.from_dask(df)\n",
    "print(ds.std())  # -> 2886.89568"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
