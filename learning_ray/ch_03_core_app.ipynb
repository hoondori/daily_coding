{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732312c0-107c-42c4-8eff-206d445ed629",
   "metadata": {},
   "source": [
    "## 간단한 미로 문제 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f42e16-e157-4f17-9c39-cda080c39524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 이산 행동/관찰 공간\n",
    "class Discrete:\n",
    "    def __init__(self, num_actions: int):\n",
    "        self.n = num_actions\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n -1)\n",
    "space = Discrete(4)\n",
    "print(space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41e9e3e5-94e7-4884-9b7c-d22e7cd932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 환경 : agent, 목표, 행동 공간, 관찰 공간\n",
    "class Environment:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.seeker, self.goal = (0,0), (4,4)\n",
    "        self.info = {'seeker' : self.seeker, 'goal': self.goal}\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "    \n",
    "    def reset(self):\n",
    "        # seeker의 위치를 초기화시키고 관찰을 반환\n",
    "        self.seeker = (0,0)\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        # seeker의 위치를 정수로 인코딩\n",
    "        return 5*self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        # 목표 도달했으면 1, 아니라면 0\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "\n",
    "    def is_done(self):\n",
    "        # 목표 도달시\n",
    "        return self.seeker == self.goal\n",
    "\n",
    "    def step(self, action):\n",
    "        # 한 방향, 한걸음 이동한 뒤에 필요한 모든 정보를 반환\n",
    "        if action == 0:  # move down\n",
    "            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\n",
    "        elif action == 1:  # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1] - 1, 0))\n",
    "        elif action == 2:  # move up\n",
    "            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\n",
    "        elif action == 3:  # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1] + 1, 4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        obs = self.get_observation()\n",
    "        rew = self.get_reward()\n",
    "        done = self.is_done()\n",
    "        return obs, rew, done, self.info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        # 환경 랜더링\n",
    "        os.system('clear')\n",
    "        grid = [['| ' for _ in range(5)] + ['|\\n'] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f0f2dd-1c57-4c14-9957-276ceaebdd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J|S| | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n",
      "\u001b[H\u001b[2J|S| | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n",
      "\u001b[H\u001b[2J| |S| | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n",
      "\u001b[H\u001b[2J| | | | | |\n",
      "| |S| | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n",
      "\u001b[H\u001b[2J| |S| | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n",
      "\u001b[H\u001b[2J|S| | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n",
      "\u001b[H\u001b[2J| |S| | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | |G|\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m random_action \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      6\u001b[0m environment\u001b[38;5;241m.\u001b[39mstep(random_action)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m environment\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "environment = Environment()\n",
    "\n",
    "while not environment.is_done():\n",
    "    random_action = environment.action_space.sample()\n",
    "    environment.step(random_action)\n",
    "    time.sleep(1)\n",
    "    environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01f198-bf2c-4ad5-bd19-b9e47255b4ac",
   "metadata": {},
   "source": [
    "## 시뮬레이션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ad42b-4c04-4b97-8099-690f50d7e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, env):\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)]\n",
    "             for _ in range(env.observation_space.n)\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):\n",
    "        # 무작위 탐색을 하거나 현재 사용 가능한 최고의 값을 활용\n",
    "        if explore and random.uniform(0, 1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce1605c4-dedd-412a-b65c-2d76359412ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        # 정해진 정책으로 환경을 롤아웃해 시뮬레이션\n",
    "        self.env = env\n",
    "        \n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):\n",
    "        # 정책을 롤아웃한 경험을 반환\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            experiences.append([state, action, reward, next_state])\n",
    "            state = next_state\n",
    "            if render:\n",
    "                time.sleep(1)\n",
    "                self.env.render()\n",
    "        return experiences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b1a4dd2-5a58-4991-90a1-bc463eb7ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "untrained_policy = Policy(environment)\n",
    "sim = Simulation(environment)\n",
    "exp = sim.rollout(untrained_policy, render=False, epsilon=1.0)\n",
    "for row in untrained_policy.state_action_table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae4400-ca2c-4ecb-81de-9c02d588b033",
   "metadata": {},
   "source": [
    "## 강화학습 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d51de939-7af5-443a-a827-63df06e103d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy, experiences, weight=0.1, discount_factor=0.9):\n",
    "    # 주어진 정책을 experience 로 업데이트한다. \n",
    "    for state, action, reward, next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action] \n",
    "        new_value = (1-weight) * value + weight * (reward + discount_factor*next_max)\n",
    "        policy.state_action_table[state][action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bb21e6d-9e7a-4fc1-87ee-9cc78a8e892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4782968999999985, 0.4304672099999939, 0.43046720999996124, 0.38742048898349213]\n",
      "[0.14753450961534276, 0.430467209999325, 0.2020971018157964, 0.012928799347924402]\n",
      "[0.03045146291083148, 0.10499094828036047, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0.5314409999999985, 0.47829689999992375, 0.4304672099999779, 0.5314409997675092]\n",
      "[0.5904899999932254, 0.1958673635189995, 0.130143164164502, 0.059591141268447015]\n",
      "[0.2957353091417675, 0, 0, 0.0]\n",
      "[0, 0.023409017042862496, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[0.5904899999999987, 0.5314409999996254, 0.4782968999977702, 0.5904899999986886]\n",
      "[0.6560999999999991, 0.4329639097816381, 0.4376382523034612, 0.29399818132012334]\n",
      "[0.693316248418272, 0.05904899999986285, 0, 0.0]\n",
      "[0.5264436831214581, 0, 0, 0.0]\n",
      "[0.0171, 0, 0, 0]\n",
      "[0.6560999999999991, 0.590489999999807, 0.5314409999975317, 0.6560999999983582]\n",
      "[0.7289999999999993, 0.537804865951427, 0.5370006555082681, 0.6899362522957431]\n",
      "[0.8099999999999605, 0.3736704147584959, 0.35054094468874525, 0.5810332416882418]\n",
      "[0.8999999999999843, 0.29848699450795935, 0.21681635791610504, 0.07921890000000001]\n",
      "[0.40951000000000004, 0.08099999999958647, 0, 0]\n",
      "[0.6560999999973421, 0.6560999999994799, 0.5904899999996429, 0.7289999999999993]\n",
      "[0.7289999999978373, 0.6560999999995345, 0.6560999999998842, 0.8099999999999993]\n",
      "[0.8099999999969708, 0.7289999999999751, 0.7289999999933445, 0.8999999999999994]\n",
      "[0.899999999999306, 0.8099999999851031, 0.8099999999994779, 0.9999999999999994]\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    # 롤아웃에서 얻은 경험을 통해 정책을 훈련한다. \n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)\n",
    "        update_policy(policy, experiences, weight, discount_factor)\n",
    "    return policy\n",
    "trained_policy = train_policy(environment)\n",
    "for row in trained_policy.state_action_table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ba10da6-8b7d-45dd-9fb6-d394e4e16721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 steps on average for a total of 10 episodes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    # 롤아웃으로 훈련한 정책 평가\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=False, explore=False)\n",
    "        steps += len(experiences)\n",
    "        \n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "    return steps / num_episodes\n",
    "\n",
    "\n",
    "evaluate_policy(environment, trained_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546efff-a03c-43b8-aa72-82b24c68d76e",
   "metadata": {},
   "source": [
    "## 레이 분산 애플리케이션 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e72b78a-c2a2-478b-afe9-49ab6a2346ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 06:02:04,279\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.10.13', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '172.30.1.49', 'raylet_ip_address': '172.30.1.49', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2024-03-08_06-02-02_175006_91576/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2024-03-08_06-02-02_175006_91576/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2024-03-08_06-02-02_175006_91576', 'metrics_export_port': 61425, 'gcs_address': '172.30.1.49:61642', 'address': '172.30.1.49:61642', 'dashboard_agent_listen_port': 52365, 'node_id': '3e7bc190fc93e22bc1d38d63b01c2911d59bce65cf009bdf6e2f60e4'})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebe50f16-1380-4d64-99f9-516521ddef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 steps on average for a total of 10 episodes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ray.remote\n",
    "class SimulationActor(Simulation):\n",
    "    def __init__(self):\n",
    "        env = Environment()\n",
    "        super().__init__(env)\n",
    "\n",
    "def train_policy_parallel(env, num_episodes=100, num_simulations=4):\n",
    "    # 병렬 정책 훈련 함수\n",
    "    policy = Policy(env)\n",
    "    simulations = [SimulationActor.remote() for _ in range(num_simulations)]\n",
    "\n",
    "    policy_ref = ray.put(policy)\n",
    "    for _ in range(num_episodes):\n",
    "        exp_list = [sim.rollout.remote(policy_ref) for sim in simulations]\n",
    "        while len(exp_list) > 0:\n",
    "            finished, exp_list = ray.wait(exp_list)\n",
    "            for exp in ray.get(finished):\n",
    "                update_policy(policy, exp)\n",
    "    return policy\n",
    "\n",
    "parallel_policy = train_policy_parallel(environment)\n",
    "evaluate_policy(environment, parallel_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2568c-74e7-4eb3-9b3b-3f0893257b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
